{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKQj6KUUKd+Z4Hq/gy8hyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barabonda/DEVOCEANYOUNG-Study-ThisIsCodingTest/blob/main/3%EC%A3%BC%EC%B0%A8/%EC%9B%94%EA%B0%84_%EB%8D%B0%EC%9D%B4%EC%BD%98_%EB%B2%95%EC%9B%90_%ED%8C%90%EA%B2%B0_%EC%98%88%EC%B8%A1_AI_%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V5N0yaw_jus",
        "outputId": "e3ae523d-c26d-4406-c441-2790b4786649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: spacy[cuda-autodetect] in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.23.5)\n",
            "Requirement already satisfied: cupy-wheel<13.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (12.3.0)\n",
            "Requirement already satisfied: cupy-cuda12x==12.3.0 in /usr/local/lib/python3.10/dist-packages (from cupy-wheel<13.0.0,>=11.0.0->spacy[cuda-autodetect]) (12.3.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x==12.3.0->cupy-wheel<13.0.0,>=11.0.0->spacy[cuda-autodetect]) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda-autodetect]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy[cuda-autodetect]) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy[cuda-autodetect]) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy[cuda-autodetect]) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy[cuda-autodetect]) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy[cuda-autodetect]) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2024-01-03 02:00:29.764320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-03 02:00:29.764443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-03 02:00:29.768513: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-03 02:00:33.108847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2024-01-03 02:00:51.591834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-03 02:00:51.591912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-03 02:00:51.593498: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-03 02:00:53.568712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U 'spacy[cuda-autodetect]'\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy requests nlpaug\n",
        "!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w35t8eyN_qM2",
        "outputId": "b5ef97cb-98ec-43de-fa23-6ac497b44b74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import itertools\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_colwidth', 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JHpS-n3AGSr",
        "outputId": "44a4ad27-8d51-4771-f549-060492c23ef2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n"
      ],
      "metadata": {
        "id": "nTm0LS7FALka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "EVmd34Y0ANdX",
        "outputId": "be0658f5-51a0-4737-99e5-e65a92e65fa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a1da497a12f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_augmentation = train['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average train facts character length (before augmentation): {avg_char_before_augmentation:.0f}')\n",
        "\n",
        "avg_word_before_augmentation = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average train facts word length (before augmentation): {avg_word_before_augmentation:.0f}')"
      ],
      "metadata": {
        "id": "OwSvyA3qAfAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_augmentation = test['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average test facts character length (before augmentation): {avg_char_before_augmentation:.0f}')\n",
        "\n",
        "avg_word_before_augmentation = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average test facts word length (before augmentation): {avg_word_before_augmentation:.0f}')"
      ],
      "metadata": {
        "id": "wkaxBhFzAqJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_apply = ['first_party', 'second_party', 'facts']"
      ],
      "metadata": {
        "id": "sj5DEH9sAr1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns_to_apply:\n",
        "    train[column] = train[column].apply(lambda x: re.sub(r'\\bWill\\b', 'Willn', x))\n",
        "\n",
        "for column in columns_to_apply:\n",
        "    test[column] = test[column].apply(lambda x: re.sub(r'\\bWill\\b', 'Willn', x))"
      ],
      "metadata": {
        "id": "gh_0o6D0Audf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replace_dict = {\n",
        "    'U. S. C.': ' USC ',\n",
        "    'U.S.C.': ' USC ',\n",
        "    'U.S.A.': ' USA ',\n",
        "    'U. S.': ' USA ',\n",
        "    'U.S.': ' USA ',\n",
        "    'US ': ' USA ',\n",
        "    'United States of America': ' USA ',\n",
        "    'United States': ' USA ',\n",
        "    'united states': ' USA '\n",
        "}"
      ],
      "metadata": {
        "id": "7ifMA_qwAwHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train.columns:\n",
        "    train[col] = train[col].replace(replace_dict, regex=True)\n",
        "\n",
        "for col in test.columns:\n",
        "    test[col] = test[col].replace(replace_dict, regex=True)"
      ],
      "metadata": {
        "id": "HuyONVNbAyHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_whitespaces_func(text):\n",
        "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()"
      ],
      "metadata": {
        "id": "eT9lFqIKA0cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "l0sw0fp-A5Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_Large_and_Large(text):\n",
        "    pattern1 = r'(?<=[A-Z])\\.+\\s+(?=[A-Z]+\\.)'\n",
        "    pattern2 = r'(?<=[A-Z])\\.+(?=[A-Z]+\\.)'\n",
        "    pattern3 = r'([A-Z])\\.'\n",
        "\n",
        "\n",
        "    result1 = re.sub(pattern1, '', text)\n",
        "    result2 = re.sub(pattern2, '', result1)\n",
        "    result3 = re.sub(pattern3, lambda match: match.group(1)+' ', result2)\n",
        "\n",
        "    return result3"
      ],
      "metadata": {
        "id": "-ETHKkXuA7PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_Large_and_Large))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_Large_and_Large))"
      ],
      "metadata": {
        "id": "xuoRC4P2A80I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "vzWDxjWBA_Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_And(text):\n",
        "    pattern1 = r'(?<=[A-Z])\\s+\\&\\s+(?=[A-Z])'\n",
        "    pattern2 = r'(?<=[A-Z])\\&(?=[A-Z])'\\\n",
        "\n",
        "    result1 = re.sub(pattern1, 'n', text)\n",
        "    result2 = re.sub(pattern2, 'n', result1)\n",
        "    result3 = result2.replace('&',' and ')\n",
        "\n",
        "    return result3"
      ],
      "metadata": {
        "id": "5BSd8nzUBA4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_And))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_And))"
      ],
      "metadata": {
        "id": "oiItFTO-BC3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "KKYvqagVBE4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_One_Large(text):\n",
        "    text = ' ' + text + ' '\n",
        "    pattern = r'(?<=\\s)[A-Z](?=\\s)'\n",
        "\n",
        "    result = re.sub(pattern, ' ', text)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "IQ2Q-9QBBGqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_One_Large))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_One_Large))"
      ],
      "metadata": {
        "id": "Fqjf_GBlBINA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "0KI4U45TBK_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hon_rep = [r'Co\\.', r'CO\\.' , r'Bd\\.', r'Mt\\.']"
      ],
      "metadata": {
        "id": "ynLG5-hSBNB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_values_in_column(column, pattern, replacement):\n",
        "    new_column = []\n",
        "    for item in tqdm(column):\n",
        "        new_item = re.sub(pattern, replacement, item)\n",
        "        new_column.append(new_item)\n",
        "    return new_column\n",
        "\n",
        "# 해당 패턴에 맞는 값들을 원하는 replacement 값으로 바꾸는 함수\n",
        "def replace_values_in_train(train, column_name, pattern, replacement):\n",
        "    train[column_name] = replace_values_in_column(train[column_name], pattern, replacement)\n",
        "    return train\n",
        "\n",
        "def replace_values_in_test(test, column_name, pattern, replacement):\n",
        "    test[column_name] = replace_values_in_column(test[column_name], pattern, replacement)\n",
        "    return test"
      ],
      "metadata": {
        "id": "Ci4NFdrFBOQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 데이터프레임에서 각 컬럼들에 대해 값을 바꿈\n",
        "for pattern, replacement in zip(Hon_rep, [' Company ', ' Company ', ' Building ', ' Mount ']):\n",
        "    train = replace_values_in_train(train, 'first_party', pattern, replacement)\n",
        "    train = replace_values_in_train(train, 'second_party', pattern, replacement)\n",
        "    train = replace_values_in_train(train, 'facts', pattern, replacement)"
      ],
      "metadata": {
        "id": "qob-gz9eBPxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터프레임에서 각 컬럼들에 대해 값을 바꿈\n",
        "for pattern, replacement in zip(Hon_rep, [' Company ', ' Company ', ' Building ', ' Mount ']):\n",
        "    test = replace_values_in_test(test, 'first_party', pattern, replacement)\n",
        "    test = replace_values_in_test(test, 'second_party', pattern, replacement)\n",
        "    test = replace_values_in_test(test, 'facts', pattern, replacement)"
      ],
      "metadata": {
        "id": "rjSX4IrXBRvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "gkkX7zwZBUGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_dot_after_inc1(text):\n",
        "    pattern = r'Inc\\.\\s+([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot, text)\n",
        "    return result\n",
        "\n",
        "def remove_dot_after_inc2(text):\n",
        "    pattern = r'Inc\\.([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot2(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot2, text)\n",
        "    return result\n",
        "\n",
        "def remove_dot_after_ltd1(text):\n",
        "    pattern = r'Ltd\\.\\s+([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot3(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot3, text)\n",
        "    return result\n",
        "\n",
        "def remove_dot_after_ltd2(text):\n",
        "    pattern = r'Ltd\\.([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot4(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot4, text)\n",
        "    return result"
      ],
      "metadata": {
        "id": "J6TPtvotBV0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc1))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc2))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd1))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd2))\n",
        "\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc1))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc2))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd1))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd2))"
      ],
      "metadata": {
        "id": "2stcojX8BYY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "0Kqdm6B6BaRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "9_VPTdLjBb3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_str_train = []\n",
        "noun_str_test = []"
      ],
      "metadata": {
        "id": "GbRo8HsfBeA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_str_train = train['facts'].apply(lambda x: [chunk.text for chunk in nlp(x).noun_chunks]).tolist()\n",
        "noun_str_test = test['facts'].apply(lambda x: [chunk.text for chunk in nlp(x).noun_chunks]).tolist()"
      ],
      "metadata": {
        "id": "j7bPE0ErBffm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_0 = train[train['first_party_winner'] == 0].copy()\n",
        "train_0 = pd.concat([train_0]*5, ignore_index=True)\n",
        "train_0['number'] = train_0['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train_0 = train_0.sort_values('number').reset_index(drop=True)\n",
        "train_0 = train_0.drop('number', axis=1)"
      ],
      "metadata": {
        "id": "7sfx0-96BhOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1 = train[train['first_party_winner'] == 1].copy()\n",
        "train_1 = pd.concat([train_1]*5, ignore_index=True)\n",
        "train_1['number'] = train_1['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train_1 = train_1.sort_values('number').reset_index(drop=True)\n",
        "train_1 = train_1.drop('number', axis=1)"
      ],
      "metadata": {
        "id": "dYZfSN_LBkuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_str_train_2 = []"
      ],
      "metadata": {
        "id": "bRJ4AYFqBoRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(noun_str_train)):\n",
        "    noun_str_train_2.append(list(set(noun_str_train[i])))"
      ],
      "metadata": {
        "id": "LDrnFFOuBqvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_facts_train_0 = []\n",
        "final_facts_train_1 = []"
      ],
      "metadata": {
        "id": "eZPZF8LwBspa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_train_0 = train[train['first_party_winner'] == 0].index.tolist()\n",
        "indices_train_1 = train[train['first_party_winner'] == 1].index.tolist()"
      ],
      "metadata": {
        "id": "wTrvCag6BuLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(indices_train_0):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', stopwords=noun_str_train_2[i])\n",
        "    final_facts_train_0.append([train['facts'][i]])\n",
        "    for j in range(4):\n",
        "        augmented_train_0 = aug.augment(train['facts'][i],1,8)\n",
        "        final_facts_train_0.append(augmented_train_0)"
      ],
      "metadata": {
        "id": "LplB556iBxjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(indices_train_1):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', stopwords=noun_str_train_2[i])\n",
        "    final_facts_train_1.append([train['facts'][i]])\n",
        "    for j in range(4):\n",
        "        augmented_train_1 = aug.augment(train['facts'][i],1,8)\n",
        "        final_facts_train_1.append(augmented_train_1)"
      ],
      "metadata": {
        "id": "FCyQHtu0BzOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_facts_train_0 = [item for sublist in final_facts_train_0 for item in sublist]\n",
        "final_facts_train_1 = [item for sublist in final_facts_train_1 for item in sublist]"
      ],
      "metadata": {
        "id": "tVOJYlgeB08I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_0['facts'] = final_facts_train_0\n",
        "train_1['facts'] = final_facts_train_1"
      ],
      "metadata": {
        "id": "sQyvO9C4B3TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train_0,train_1]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "1MrVoy0yCF-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['number'] = train['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train = train.sort_values('ID').reset_index(drop=True)\n",
        "train = train.drop('number', axis=1)\n",
        "train['ID'] = train.index.map(lambda x: f'TRAIN_{x:04}')"
      ],
      "metadata": {
        "id": "UE2HoQSDCHQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replace_dict = {\n",
        "\n",
        "    'et. al.': ' ',\n",
        "    'et. al': ' ',\n",
        "    'et al.': ' ',\n",
        "    'et al': ' ',\n",
        "\n",
        "    'at. al.': ' ',\n",
        "    'at. al': ' ',\n",
        "    'at al.': ' ',\n",
        "    'at al': ' ',\n",
        "\n",
        "    'et. ux.': ' ',\n",
        "    'et. ux': ' ',\n",
        "    'et ux.': ' ',\n",
        "    'et ux': ' ',\n",
        "\n",
        "    'et. ex.': ' ',\n",
        "    'et. ex': ' ',\n",
        "    'et ex.': ' ',\n",
        "    'et ex': ' ',\n",
        "\n",
        "    'ex. re.': ' ',\n",
        "    'ex. re': ' ',\n",
        "    'ex re.': ' ',\n",
        "    'ex re': ' ',\n",
        "\n",
        "    'et. re.': ' ',\n",
        "    'et. re': ' ',\n",
        "    'et re.': ' ',\n",
        "    'et re': ' ',\n",
        "\n",
        "    'et. seq.': ' ',\n",
        "    'et. seq': ' ',\n",
        "    'et seq.': ' ',\n",
        "    'et seq': ' ',\n",
        "\n",
        "    'et. vir.': ' ',\n",
        "    'et. vir': ' ',\n",
        "    'et vir.': ' ',\n",
        "    'et vir': ' ',\n",
        "\n",
        "    'ex. rel.': ' ',\n",
        "    'ex. rel': ' ',\n",
        "    'ex rel.': ' ',\n",
        "    'ex rel': ' ',\n",
        "    'etc' : ' '\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "BrTxBMAdCKlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train.columns:\n",
        "    train[col] = train[col].replace(replace_dict, regex=True)\n",
        "\n",
        "for col in test.columns:\n",
        "    test[col] = test[col].replace(replace_dict, regex=True)\n"
      ],
      "metadata": {
        "id": "hVmzy65cCMwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "pgrelHPnLHDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Honor = [r'Mr.', r'Mrs.', r'Miss.', r'Dr.', r'Rev.', r'Prof.', r'Capt.', r'Sgt.', r'St.', r'Sr.', r'Jr.', r'Ms.', r'No.']"
      ],
      "metadata": {
        "id": "LOHg_Ik4LKmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(train))):\n",
        "    for k in Honor:\n",
        "        train.loc[i, 'facts'] = \" \" + train.loc[i, 'facts'] + \" \"\n",
        "        train.loc[i, 'facts'] = train.loc[i, 'facts'].replace(k,' ')\n",
        "        train.loc[i, 'facts'] = re.sub(r'\\s+', ' ', train.loc[i, 'facts'])\n",
        "        train.loc[i, 'facts'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'facts'])\n",
        "\n",
        "        train.loc[i, 'first_party'] = \" \" + train.loc[i, 'first_party'] + \" \"\n",
        "        train.loc[i, 'first_party'] = train.loc[i, 'first_party'].replace(k,' ')\n",
        "        train.loc[i, 'first_party'] = re.sub(r'\\s+', ' ', train.loc[i, 'first_party'])\n",
        "        train.loc[i, 'first_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'first_party'])\n",
        "\n",
        "        train.loc[i, 'second_party'] = \" \" + train.loc[i, 'second_party'] + \" \"\n",
        "        train.loc[i, 'second_party'] = train.loc[i, 'second_party'].replace(k,' ')\n",
        "        train.loc[i, 'second_party'] = re.sub(r'\\s+', ' ', train.loc[i, 'second_party'])\n",
        "        train.loc[i, 'second_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'second_party'])\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(test))):\n",
        "    for k in Honor:\n",
        "        test.loc[i, 'facts'] = \" \" + test.loc[i, 'facts'] + \" \"\n",
        "        test.loc[i, 'facts'] = test.loc[i, 'facts'].replace(k,' ')\n",
        "        test.loc[i, 'facts'] = re.sub(r'\\s+', ' ', test.loc[i, 'facts'])\n",
        "        test.loc[i, 'facts'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'facts'])\n",
        "\n",
        "        test.loc[i, 'first_party'] = \" \" + test.loc[i, 'first_party'] + \" \"\n",
        "        test.loc[i, 'first_party'] = test.loc[i, 'first_party'].replace(k,' ')\n",
        "        test.loc[i, 'first_party'] = re.sub(r'\\s+', ' ', test.loc[i, 'first_party'])\n",
        "        test.loc[i, 'first_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'first_party'])\n",
        "\n",
        "        test.loc[i, 'second_party'] = \" \" + test.loc[i, 'second_party'] + \" \"\n",
        "        test.loc[i, 'second_party'] = test.loc[i, 'second_party'].replace(k,' ')\n",
        "        test.loc[i, 'second_party'] = re.sub(r'\\s+', ' ', test.loc[i, 'second_party'])\n",
        "        test.loc[i, 'second_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'second_party'])"
      ],
      "metadata": {
        "id": "wyyXgb8ILMe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "Iwb_gqXtLOKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_train = []\n",
        "sentence_test = []"
      ],
      "metadata": {
        "id": "QqYT1Er5LQ7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['facts'] = train['facts'].apply(lambda text: ' '.join([str(sent) + ' [SEP]' for sent in nlp(text).sents]))\n",
        "test['facts'] = test['facts'].apply(lambda text: ' '.join([str(sent) + ' [SEP]' for sent in nlp(text).sents]))"
      ],
      "metadata": {
        "id": "A5zt9wyULS7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_preprocessing = train['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average train facts character length (before preprocessing): {avg_char_before_preprocessing:.0f}')\n",
        "\n",
        "avg_word_before_preprocessing = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average train facts word length (before preprocessing): {avg_word_before_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "4lYuBPeaLUyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_preprocessing = test['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average test facts character length (before preprocessing): {avg_char_before_preprocessing:.0f}')\n",
        "\n",
        "avg_word_before_preprocessing = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average test facts word length (before preprocessing): {avg_word_before_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "ivjDntz7LXN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags_func(text):\n",
        "    return BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "\n",
        "def remove_url_func(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "\n",
        "def remove_accented_chars_func(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def remove_punctuation_func(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "\n",
        "\n",
        "def remove_irr_char_func(text):\n",
        "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "\n",
        "def remove_extra_whitespaces_func(text):\n",
        "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
        "\n",
        "def remove_english_stopwords_func(text):\n",
        "    t = [token for token in text if token.lower() not in stopwords.words(\"english\")]\n",
        "    text = ' '.join(t)\n",
        "    return text\n",
        "\n",
        "def norm_lemm_v_a_func(text):\n",
        "    words1 = word_tokenize(text)\n",
        "    text1 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='v') for word in words1])\n",
        "    words2 = word_tokenize(text1)\n",
        "    text2 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='a') for word in words2])\n",
        "    return text2\n",
        "\n",
        "def remove_single_char_func(text, threshold=1):\n",
        "    words = word_tokenize(text)\n",
        "    text = ' '.join([word for word in words if len(word) > threshold])\n",
        "    return text"
      ],
      "metadata": {
        "id": "910fFPtkLY7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.str.lower())\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_html_tags_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_url_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_accented_chars_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_punctuation_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_irr_char_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "train['facts'] = train['facts'].apply(lambda x: x.replace('cls', 'CLS').replace('sep', 'SEP'))"
      ],
      "metadata": {
        "id": "PffkxAsDLcdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.str.lower())\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_html_tags_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_url_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_accented_chars_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_punctuation_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_irr_char_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test['facts'] = test['facts'].apply(lambda x: x.replace('cls', 'CLS').replace('sep', 'SEP'))"
      ],
      "metadata": {
        "id": "j74P8IKgLeMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(word_tokenize))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(word_tokenize))"
      ],
      "metadata": {
        "id": "ZZZf63FNLf7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_english_stopwords_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_english_stopwords_func))"
      ],
      "metadata": {
        "id": "_fegu6R3LhuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(norm_lemm_v_a_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(norm_lemm_v_a_func))"
      ],
      "metadata": {
        "id": "Tx2LBmfBLjnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_single_char_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_single_char_func))"
      ],
      "metadata": {
        "id": "DebKRXm8LqP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_after_preprocessing = train['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average train facts character length (after preprocessing): {avg_char_after_preprocessing:.0f}')\n",
        "\n",
        "avg_word_after_preprocessing = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average train facts word length (after preprocessing): {avg_word_after_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "3v3vt8sULuSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_after_preprocessing = test['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average test facts character length (after preprocessing): {avg_char_after_preprocessing:.0f}')\n",
        "\n",
        "avg_word_after_preprocessing = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average test facts word length (after preprocessing): {avg_word_after_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "OonnK_GpLudj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_word_func(text, n_words=25):\n",
        "    words = word_tokenize(text)\n",
        "    fdist = FreqDist(words)\n",
        "\n",
        "    n_words = n_words\n",
        "\n",
        "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
        "                             'Frequency': fdist.values()})\n",
        "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False).head(n_words)\n",
        "\n",
        "    return df_fdist\n",
        "\n",
        "def label_func(winner):\n",
        "    if winner == 0:\n",
        "        return 'second_party'\n",
        "    elif winner == 1:\n",
        "        return 'first_party'\n",
        "\n",
        "def least_common_word_func(text, n_words=25):\n",
        "    words = word_tokenize(text)\n",
        "    fdist = FreqDist(words)\n",
        "\n",
        "    n_words = n_words\n",
        "\n",
        "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
        "                             'Frequency': fdist.values()})\n",
        "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False).tail(n_words)\n",
        "\n",
        "    return df_fdist"
      ],
      "metadata": {
        "id": "Ti4eUyYKLwMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = pd.concat([train.iloc[:,:-1],test]).reset_index(drop=True)\n",
        "train_copy = train.copy()\n",
        "test_copy = test.copy()"
      ],
      "metadata": {
        "id": "0_gzwdkpLy2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_corpus = df_copy['facts'].str.cat(sep=' ')\n",
        "\n",
        "df_most_common_words_text_corpus = most_common_word_func(text_corpus)\n",
        "\n",
        "df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])].head(10)"
      ],
      "metadata": {
        "id": "95RSP7AbL0TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(11,7))\n",
        "plt.bar(df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])]['Word'],\n",
        "        df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])]['Frequency'])\n",
        "\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "plt.xlabel('Most common Words')\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Frequency distribution of the 25 most common words\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TeJvX45L16M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_copy['Label'] = train_copy['first_party_winner'].apply(label_func)"
      ],
      "metadata": {
        "id": "XYz3rrxPL51l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = list(train_copy.columns)\n",
        "cols = [cols[-1]] + cols[:-1]\n",
        "train_copy = train_copy[cols]"
      ],
      "metadata": {
        "id": "7aVaJSzJL8GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_party = train_copy[(train_copy[\"Label\"] == 'first_party')]['facts']\n",
        "second_party = train_copy[(train_copy[\"Label\"] == 'second_party')]['facts']"
      ],
      "metadata": {
        "id": "xIGSEvVtL9fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_corpus_first_party = first_party.str.cat(sep=' ')\n",
        "text_corpus_second_party = second_party.str.cat(sep=' ')"
      ],
      "metadata": {
        "id": "N8Ti4cdgL_e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_most_common_words_text_corpus_first_party = most_common_word_func(text_corpus_first_party)\n",
        "df_most_common_words_text_corpus_second_party = most_common_word_func(text_corpus_second_party)"
      ],
      "metadata": {
        "id": "hdKlTQiOMA-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splited_data = [df_most_common_words_text_corpus_first_party[~df_most_common_words_text_corpus_first_party['Word'].isin(['SEP', 'CLS'])],\n",
        "                df_most_common_words_text_corpus_second_party[~df_most_common_words_text_corpus_second_party['Word'].isin(['SEP', 'CLS'])]]\n",
        "\n",
        "color_list = ['green', 'red']\n",
        "title_list = ['First party', 'Second party']\n",
        "\n",
        "\n",
        "for item in range(2):\n",
        "    plt.figure(figsize=(11,7))\n",
        "    plt.bar(splited_data[item]['Word'],\n",
        "            splited_data[item]['Frequency'],\n",
        "            color=color_list[item])\n",
        "    plt.xticks(rotation = 45)\n",
        "    plt.xlabel('Most common Words')\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Frequency distribution of the 25 most common words\")\n",
        "    plt.suptitle(title_list[item], fontsize=15)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eXf46aBhMCk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_02 = train.copy()"
      ],
      "metadata": {
        "id": "SVZXFYIVMExQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_02['first_party'], train_02['second_party'] = train_02['second_party'], train_02['first_party']\n",
        "train_02['first_party_winner'] = 1 - train_02['first_party_winner']"
      ],
      "metadata": {
        "id": "VS2KvKHjMHFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_02 = pd.concat([train, train_02], ignore_index=True)"
      ],
      "metadata": {
        "id": "iopcn5CGMJOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_02['number'] = train_02['ID'].str.replace('TRAIN_', '').astype(int)"
      ],
      "metadata": {
        "id": "IDToNHb8MLXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_02 = train_02.sort_values(['number', 'first_party_winner']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rX2LRp2MMNLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_02.drop('number', axis=1)"
      ],
      "metadata": {
        "id": "4ABJkrpIMO2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['ID'] = 'TRAIN_' + train.index.map(lambda x: f'{x:04}')"
      ],
      "metadata": {
        "id": "ZB4A6B1nMQc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns_to_apply:\n",
        "    train[column] = train[column].apply(lambda x: re.sub(r'\\bSEP\\b', '[SEP]', x))\n",
        "\n",
        "for column in columns_to_apply:\n",
        "    test[column] = test[column].apply(lambda x: re.sub(r'\\bSEP\\b', '[SEP]', x))"
      ],
      "metadata": {
        "id": "WhIk8Z8AMR5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['facts'] = '[CLS] ' + train['first_party'] + ' [SEP] ' + train['second_party'] + ' [SEP] ' + train['facts']\n",
        "test['facts'] = '[CLS] ' + test['first_party'] + ' [SEP] ' + test['second_party'] + ' [SEP] ' + test['facts']"
      ],
      "metadata": {
        "id": "2eNXZscxMTWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train_last_3.csv', index = False)\n",
        "test.to_csv('test_last_3.csv', index = False)"
      ],
      "metadata": {
        "id": "d3VIHy4WMXjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('./train_last_3.csv')\n",
        "test = pd.read_csv('./test_last_3.csv')"
      ],
      "metadata": {
        "id": "PsawMbdhMZpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.nn import BCEWithLogitsLoss"
      ],
      "metadata": {
        "id": "6OsHY6UKMdNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"sileod/deberta-v3-base-tasksource-nli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "config = AutoConfig.from_pretrained(model_type)\n",
        "model_01 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_02 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_03 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)"
      ],
      "metadata": {
        "id": "GAnmDUAXMfEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts, tokenizer, max_len=352):\n",
        "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, add_special_tokens=False , return_tensors='pt')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenize_texts(text, self.tokenizer, self.max_len)\n",
        "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "uiDWhmwFMhMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts, tokenizer, max_len=352):\n",
        "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, add_special_tokens=False , return_tensors='pt')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenize_texts(text, self.tokenizer, self.max_len)\n",
        "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "jHpD0pCoMm1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_folds = [1, 2, 3]\n",
        "train_indices = []\n",
        "val_indices = []"
      ],
      "metadata": {
        "id": "MPQirUSbMojV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_index = train.iloc[list(range(0, 24780, 10))]"
      ],
      "metadata": {
        "id": "zz0jQj7gMrVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_val_splits = list(skf.split(train_index, train_index['first_party_winner']))\n"
      ],
      "metadata": {
        "id": "S3VIBQjdMsxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k_fold in k_folds:\n",
        "    train_train =  list(train_index.iloc[train_val_splits[k_fold][0],:].index)\n",
        "    train_val =  list(train_index.iloc[train_val_splits[k_fold][1],:].index)\n",
        "\n",
        "    train_list = train_train[:]\n",
        "    val_list = train_val[:]\n",
        "\n",
        "    for num in train_train:\n",
        "        for i in range(1, 10):\n",
        "            new_value = num + i\n",
        "            train_list.append(new_value)\n",
        "\n",
        "    for num in train_val:\n",
        "        for i in range(1, 10):\n",
        "            new_value = num + i\n",
        "            val_list.append(new_value)\n",
        "\n",
        "    train_list = sorted(train_list)\n",
        "    val_list = sorted(val_list)\n",
        "\n",
        "    train_indices.append(sorted(train_list))\n",
        "    val_indices.append(sorted(val_list))"
      ],
      "metadata": {
        "id": "OI2OdgBOMv83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_01 = train.iloc[train_indices[0]]\n",
        "val_data_01 = train.iloc[val_indices[0]]\n",
        "\n",
        "train_data_02 = train.iloc[train_indices[1]]\n",
        "val_data_02 = train.iloc[val_indices[1]]\n",
        "\n",
        "train_data_03 = train.iloc[train_indices[2]]\n",
        "val_data_03 = train.iloc[val_indices[2]]"
      ],
      "metadata": {
        "id": "N3oFDMLiMwLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_01 = NewsDataset(train_data_01['facts'].to_numpy(), train_data_01['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_01 = NewsDataset(val_data_01['facts'].to_numpy(), val_data_01['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_02 = NewsDataset(train_data_02['facts'].to_numpy(), train_data_02['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_02 = NewsDataset(val_data_02['facts'].to_numpy(), val_data_02['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_03 = NewsDataset(train_data_03['facts'].to_numpy(), train_data_03['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_03 = NewsDataset(val_data_03['facts'].to_numpy(), val_data_03['first_party_winner'].to_numpy(), tokenizer, max_len=352)"
      ],
      "metadata": {
        "id": "cKqR7nDyMxst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_01 = DataLoader(train_dataset_01, batch_size=32, shuffle=True)\n",
        "val_loader_01 = DataLoader(val_dataset_01, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_02 = DataLoader(train_dataset_02, batch_size=32, shuffle=True)\n",
        "val_loader_02 = DataLoader(val_dataset_02, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_03 = DataLoader(train_dataset_03, batch_size=32, shuffle=True)\n",
        "val_loader_03 = DataLoader(val_dataset_03, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "n-bZk9hhMzST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_01.to(device)\n",
        "model_02.to(device)\n",
        "model_03.to(device)"
      ],
      "metadata": {
        "id": "KN8BSQGPM07e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(torch.nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "metadata": {
        "id": "6gE_MH6aM3EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_01 = AdamW(model_01.parameters(), lr=1e-5)\n",
        "optimizer_02 = AdamW(model_02.parameters(), lr=1e-5)\n",
        "optimizer_03 = AdamW(model_03.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "UKp2p6PQM6LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "num_classes = 2\n",
        "smoothing = 0.01\n",
        "criterion = LabelSmoothingLoss(classes=num_classes, smoothing=smoothing).to(device)"
      ],
      "metadata": {
        "id": "9Hi2UTFGM8Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training and validation\n",
        "def train(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval(model, data_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predictions.cpu().numpy())\n",
        "    return y_true, y_pred"
      ],
      "metadata": {
        "id": "eIgaILztM9c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03]\n",
        "model_loader = [model_01, model_02, model_03]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03]\n",
        "\n",
        "\n",
        "best_accuracy_01 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[0], train_loader[0], optimizer_loader[0], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[0], train_loader[0], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[0], val_loader[0], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_01:\n",
        "        best_accuracy_01 = val_accuracy\n",
        "        torch.save(model_loader[0].state_dict(), f'model_loader[0]_{k_folds[0]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ftPVM1YCNAAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "DniJxb-3NB5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_01.load_state_dict(torch.load(f'model_loader[0]_1.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_01, test_loader, device)\n",
        "submit['first_party_winner_01'] = test_predictions"
      ],
      "metadata": {
        "id": "_BTiySfjNFkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01 = submit.copy()"
      ],
      "metadata": {
        "id": "3skMRJiwNG46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01.to_csv('submit_01_version2.csv', index = False)"
      ],
      "metadata": {
        "id": "IQ4pAF_ZNIil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03]\n",
        "model_loader = [model_01, model_02, model_03]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03]\n",
        "\n",
        "\n",
        "best_accuracy_02 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[1], train_loader[1], optimizer_loader[1], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[1], train_loader[1], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[1], val_loader[1], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_02:\n",
        "        best_accuracy_02 = val_accuracy\n",
        "        torch.save(model_loader[1].state_dict(), f'model_loader[1]_{k_folds[1]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "buAhHzkgNKmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "zXzg_nWcNMeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_02.load_state_dict(torch.load(f'model_loader[1]_2.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_02, test_loader, device)\n",
        "submit['first_party_winner_02'] = test_predictions"
      ],
      "metadata": {
        "id": "-3nEUO9hNPN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_02 = submit.copy()"
      ],
      "metadata": {
        "id": "eeM55z_4NQz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_02.to_csv('submit_02_version2.csv', index = False)"
      ],
      "metadata": {
        "id": "guRllERKNTEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03]\n",
        "model_loader = [model_01, model_02, model_03]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03]\n",
        "\n",
        "\n",
        "best_accuracy_03 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[2], train_loader[2], optimizer_loader[2], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[2], train_loader[2], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[2], val_loader[2], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_03:\n",
        "        best_accuracy_03 = val_accuracy\n",
        "        torch.save(model_loader[2].state_dict(), f'model_loader[2]_{k_folds[2]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "h9rMDtu7NU3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy_03"
      ],
      "metadata": {
        "id": "pjlE1ZhrNWfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "qBeCab0zNZI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_03.load_state_dict(torch.load(f'model_loader[2]_3.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_03, test_loader, device)\n",
        "submit['first_party_winner_03'] = test_predictions"
      ],
      "metadata": {
        "id": "Ccshqt6qNcLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_03 = submit.copy()"
      ],
      "metadata": {
        "id": "8T6J3GhENd7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_03.to_csv('submit_03_version2.csv', index = False)"
      ],
      "metadata": {
        "id": "05cvl0HMNh7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/data/train_last_3.csv')\n",
        "test = pd.read_csv('/data/test_last_3.csv')"
      ],
      "metadata": {
        "id": "PuPfrET1NjqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"sileod/deberta-v3-base-tasksource-nli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "config = AutoConfig.from_pretrained(model_type)\n",
        "model_01 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_02 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_03 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_04 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_05 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)"
      ],
      "metadata": {
        "id": "G-OxDke1Nlms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts, tokenizer, max_len=352):\n",
        "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, add_special_tokens=False , return_tensors='pt')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenize_texts(text, self.tokenizer, self.max_len)\n",
        "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "R2_ed_XzNnWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_folds = [0, 1, 2, 3, 4]\n",
        "train_indices = []\n",
        "val_indices = []"
      ],
      "metadata": {
        "id": "Zl_P5QDSNpQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_index = train.iloc[list(range(0, 24780, 10))]"
      ],
      "metadata": {
        "id": "6MFX-EzKNq4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_val_splits = list(skf.split(train_index, train_index['first_party_winner']))"
      ],
      "metadata": {
        "id": "VYUseRimNsaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k_fold in k_folds:\n",
        "    train_train =  list(train_index.iloc[train_val_splits[k_fold][0],:].index)\n",
        "    train_val =  list(train_index.iloc[train_val_splits[k_fold][1],:].index)\n",
        "\n",
        "    train_list = train_train[:]\n",
        "    val_list = train_val[:]\n",
        "\n",
        "    for num in train_train:\n",
        "        for i in range(1, 10):\n",
        "            new_value = num + i\n",
        "            train_list.append(new_value)\n",
        "\n",
        "    for num in train_val:\n",
        "        for i in range(1, 10):\n",
        "            new_value = num + i\n",
        "            val_list.append(new_value)\n",
        "\n",
        "    train_list = sorted(train_list)\n",
        "    val_list = sorted(val_list)\n",
        "\n",
        "    train_indices.append(sorted(train_list))\n",
        "    val_indices.append(sorted(val_list))"
      ],
      "metadata": {
        "id": "pA49SB8hNtsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_01 = train.iloc[train_indices[0]]\n",
        "val_data_01 = train.iloc[val_indices[0]]\n",
        "\n",
        "train_data_02 = train.iloc[train_indices[1]]\n",
        "val_data_02 = train.iloc[val_indices[1]]\n",
        "\n",
        "train_data_03 = train.iloc[train_indices[2]]\n",
        "val_data_03 = train.iloc[val_indices[2]]\n",
        "\n",
        "train_data_04 = train.iloc[train_indices[3]]\n",
        "val_data_04 = train.iloc[val_indices[3]]\n",
        "\n",
        "train_data_05 = train.iloc[train_indices[4]]\n",
        "val_data_05 = train.iloc[val_indices[4]]"
      ],
      "metadata": {
        "id": "nPkQiqTGNvat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_01 = NewsDataset(train_data_01['facts'].to_numpy(), train_data_01['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_01 = NewsDataset(val_data_01['facts'].to_numpy(), val_data_01['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_02 = NewsDataset(train_data_02['facts'].to_numpy(), train_data_02['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_02 = NewsDataset(val_data_02['facts'].to_numpy(), val_data_02['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_03 = NewsDataset(train_data_03['facts'].to_numpy(), train_data_03['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_03 = NewsDataset(val_data_03['facts'].to_numpy(), val_data_03['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_04 = NewsDataset(train_data_04['facts'].to_numpy(), train_data_04['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_04 = NewsDataset(val_data_04['facts'].to_numpy(), val_data_04['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_05 = NewsDataset(train_data_05['facts'].to_numpy(), train_data_05['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_05 = NewsDataset(val_data_05['facts'].to_numpy(), val_data_05['first_party_winner'].to_numpy(), tokenizer, max_len=352)"
      ],
      "metadata": {
        "id": "0JLpqmx1NxXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_01 = DataLoader(train_dataset_01, batch_size=32, shuffle=True)\n",
        "val_loader_01 = DataLoader(val_dataset_01, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_02 = DataLoader(train_dataset_02, batch_size=32, shuffle=True)\n",
        "val_loader_02 = DataLoader(val_dataset_02, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_03 = DataLoader(train_dataset_03, batch_size=32, shuffle=True)\n",
        "val_loader_03 = DataLoader(val_dataset_03, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_04 = DataLoader(train_dataset_04, batch_size=32, shuffle=True)\n",
        "val_loader_04 = DataLoader(val_dataset_04, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_05 = DataLoader(train_dataset_05, batch_size=32, shuffle=True)\n",
        "val_loader_05 = DataLoader(val_dataset_05, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "FvhDUCBYNzCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_01.to(device)\n",
        "model_02.to(device)\n",
        "model_03.to(device)\n",
        "model_04.to(device)\n",
        "model_05.to(device)"
      ],
      "metadata": {
        "id": "aNCKUPjwN1A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(torch.nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "metadata": {
        "id": "0ufTOxpWN26B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_01 = AdamW(model_01.parameters(), lr=1e-5)\n",
        "optimizer_02 = AdamW(model_02.parameters(), lr=1e-5)\n",
        "optimizer_03 = AdamW(model_03.parameters(), lr=1e-5)\n",
        "optimizer_04 = AdamW(model_04.parameters(), lr=1e-5)\n",
        "optimizer_05 = AdamW(model_05.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "1epiBkNnN4-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "num_classes = 2\n",
        "smoothing = 0.01\n",
        "criterion = LabelSmoothingLoss(classes=num_classes, smoothing=smoothing).to(device)"
      ],
      "metadata": {
        "id": "BwWpGROuN7Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training and validation\n",
        "def train(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval(model, data_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predictions.cpu().numpy())\n",
        "    return y_true, y_pred"
      ],
      "metadata": {
        "id": "W3B3yKdxN9Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03, train_loader_04, train_loader_05]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03, val_loader_04, val_loader_05]\n",
        "model_loader = [model_01, model_02, model_03, model_04, model_05]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03, optimizer_04, optimizer_05]\n",
        "\n",
        "\n",
        "best_accuracy_01 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[0], train_loader[0], optimizer_loader[0], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[0], train_loader[0], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[0], val_loader[0], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_01:\n",
        "        best_accuracy_01 = val_accuracy\n",
        "        torch.save(model_loader[0].state_dict(), f'model_fine_loader[0]_{k_folds[0]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "QStIBUT_N_dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy_01"
      ],
      "metadata": {
        "id": "abD9cH3xOBEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "ln2bdljGODG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_01.load_state_dict(torch.load(f'model_fine_loader[0]_1.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_01, test_loader, device)\n",
        "submit['first_party_winner_01'] = test_predictions"
      ],
      "metadata": {
        "id": "0VJtxRV0OE1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01 = submit.copy()"
      ],
      "metadata": {
        "id": "9S2d09C3OGSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01.to_csv('submit_01_version3.csv', index = False)"
      ],
      "metadata": {
        "id": "B4BsaAFzOHyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03, train_loader_04, train_loader_05]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03, val_loader_04, val_loader_05]\n",
        "model_loader = [model_01, model_02, model_03, model_04, model_05]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03, optimizer_04, optimizer_05]\n",
        "\n",
        "\n",
        "best_accuracy_02 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[1], train_loader[1], optimizer_loader[1], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[1], train_loader[1], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[1], val_loader[1], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_02:\n",
        "        best_accuracy_02 = val_accuracy\n",
        "        torch.save(model_loader[1].state_dict(), f'model_fine_loader[1]_{k_folds[1]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BKEhixTaOJaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy_02"
      ],
      "metadata": {
        "id": "wJUGrY6jOLhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "Kxvnb1PiOM4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_02.load_state_dict(torch.load(f'model_fine_loader[1]_2.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_02, test_loader, device)\n",
        "submit['first_party_winner_02'] = test_predictions"
      ],
      "metadata": {
        "id": "Ww4IgAd7OOVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_02 = submit.copy()"
      ],
      "metadata": {
        "id": "_o_7wRhcOQWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_02.to_csv('submit_02_version3.csv', index = False)"
      ],
      "metadata": {
        "id": "4uzTWQrEOR09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03, train_loader_04, train_loader_05]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03, val_loader_04, val_loader_05]\n",
        "model_loader = [model_01, model_02, model_03, model_04, model_05]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03, optimizer_04, optimizer_05]\n",
        "\n",
        "\n",
        "best_accuracy_03 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[2], train_loader[2], optimizer_loader[2], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[2], train_loader[2], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[2], val_loader[2], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_03:\n",
        "        best_accuracy_03 = val_accuracy\n",
        "        torch.save(model_loader[2].state_dict(), f'model_fine_loader[2]_{k_folds[2]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "I0OSK0LfOUd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy_03"
      ],
      "metadata": {
        "id": "hMr9BBrgOWre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "ENLNSJzQOYgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_03.load_state_dict(torch.load(f'model_fine_loader[2]_3.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_03, test_loader, device)\n",
        "submit['first_party_winner_03'] = test_predictions"
      ],
      "metadata": {
        "id": "uypRamXbOZss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_03 = submit.copy()"
      ],
      "metadata": {
        "id": "R5vYlffXObXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_03.to_csv('submit_03_version3.csv', index = False)"
      ],
      "metadata": {
        "id": "dEq3G5CgOdiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03, train_loader_04, train_loader_05]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03, val_loader_04, val_loader_05]\n",
        "model_loader = [model_01, model_02, model_03, model_04, model_05]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03, optimizer_04, optimizer_05]\n",
        "\n",
        "\n",
        "best_accuracy_04 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[3], train_loader[3], optimizer_loader[3], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[3], train_loader[3], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[3], val_loader[3], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_04:\n",
        "        best_accuracy_04 = val_accuracy\n",
        "        torch.save(model_loader[3].state_dict(), f'model_fine_loader[3]_{k_folds[3]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "M9-5DgmVOfo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy_04"
      ],
      "metadata": {
        "id": "2F5Gii5QOis2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "PkIZyV40OkIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_04.load_state_dict(torch.load(f'model_fine_loader[3]_4.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_04, test_loader, device)\n",
        "submit['first_party_winner_04'] = test_predictions"
      ],
      "metadata": {
        "id": "lfPosiAOOlpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_04 = submit.copy()"
      ],
      "metadata": {
        "id": "1i7A33WbOnB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_04.to_csv('submit_04_version3.csv', index = False)"
      ],
      "metadata": {
        "id": "Ee1tom36OpAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03, train_loader_04, train_loader_05]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03, val_loader_04, val_loader_05]\n",
        "model_loader = [model_01, model_02, model_03, model_04, model_05]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03, optimizer_04, optimizer_05]\n",
        "\n",
        "\n",
        "best_accuracy_05 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[4], train_loader[4], optimizer_loader[4], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[4], train_loader[4], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[4], val_loader[4], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_05:\n",
        "        best_accuracy_05 = val_accuracy\n",
        "        torch.save(model_loader[4].state_dict(), f'model_fine_loader[4]_{k_folds[4]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "j0uOcnZ2OqmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy_05"
      ],
      "metadata": {
        "id": "PfHXR7JlOspz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "dhLYGV33Ovdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_05.load_state_dict(torch.load(f'model_fine_loader[4]_4.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_05, test_loader, device)\n",
        "submit['first_party_winner_05'] = test_predictions"
      ],
      "metadata": {
        "id": "5a6gX4wXOx-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_05 = submit.copy()"
      ],
      "metadata": {
        "id": "E6-fDEbROzw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_05.to_csv('./data/submit_05_version4.csv', index = False)"
      ],
      "metadata": {
        "id": "XopgqB0KO1ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode"
      ],
      "metadata": {
        "id": "w7H2xQ4jO3ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01 = pd.read_csv('./data/submit_01_version4.csv')\n",
        "submit_02 = pd.read_csv('./data/submit_02_version4.csv')\n",
        "submit_03 = pd.read_csv('./data/submit_03_version4.csv')\n",
        "submit_04 = pd.read_csv('./data/submit_04_version4.csv')\n",
        "submit_05 = pd.read_csv('./data/submit_05_version4.csv')\n",
        "submit_06 = pd.read_csv('/data/submit_01_version2.csv')\n",
        "submit_07 = pd.read_csv('/data/submit_02_version2.csv')\n",
        "submit_08 = pd.read_csv('/data/submit_03_version2.csv')"
      ],
      "metadata": {
        "id": "35FrI2uMO5Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_06.columns = ['ID','first_party_winner','first_party_winner_06']\n",
        "submit_07.columns = ['ID','first_party_winner','first_party_winner_07']\n",
        "submit_08.columns = ['ID','first_party_winner','first_party_winner_08']"
      ],
      "metadata": {
        "id": "zKbCc46uO6zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.merge(submit_01, submit_02, on=['ID', 'first_party_winner'])\n",
        "submission = pd.merge(submission, submit_03, on=['ID', 'first_party_winner'])\n",
        "submission = pd.merge(submission, submit_04, on=['ID', 'first_party_winner'])\n",
        "submission = pd.merge(submission, submit_05, on=['ID', 'first_party_winner'])\n",
        "submission = pd.merge(submission, submit_06, on=['ID', 'first_party_winner'])\n",
        "submission = pd.merge(submission, submit_07, on=['ID', 'first_party_winner'])\n",
        "submission = pd.merge(submission, submit_08, on=['ID', 'first_party_winner'])"
      ],
      "metadata": {
        "id": "4Q-34luuO8NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission['first_party_winner'] = submission[['first_party_winner_01','first_party_winner_02', 'first_party_winner_03','first_party_winner_04','first_party_winner_05','first_party_winner_06','first_party_winner_07','first_party_winner_08']].mode(axis=1)[0]"
      ],
      "metadata": {
        "id": "AaeA9G56O9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission['first_party_winner'] = submission['first_party_winner'].astype(int)"
      ],
      "metadata": {
        "id": "W3IugdjLO_em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "8PXitLqOPBKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = submission.iloc[:,:2]"
      ],
      "metadata": {
        "id": "9qPo_yVMPC07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('submission_minsu_version_04.csv', index = False)"
      ],
      "metadata": {
        "id": "C5cMbcK4PEWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lB2TQ00VPGUJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}